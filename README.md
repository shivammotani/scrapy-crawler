# ğŸ•·ï¸ Product Crawler

This is a flexible and extensible product URL crawler built with **Scrapy**. It supports crawling multiple e-commerce websites using a common base spider, with domain-specific logic handled by individual spider handlers.

---

## ğŸš€ Features

- Crawl multiple e-commerce sites (e.g. Nykaa, TataCliq, Westside, Virgio)
- Extract and normalize product URLs
- Smart filtering using domain-specific product/collection patterns
- Pagination support (cursor or page-based)
- Modular architecture using a spider factory
- Output via Scrapy pipeline (e.g. per-domain CSVs or JSONs)

---

## ğŸ—‚ï¸ Project Structure

```
scrapy-crawler/
â”œâ”€â”€ product_crawler/
â”‚  â”œâ”€â”€ spiders/
â”‚  â”‚   â”œâ”€â”€ product_spider.py        # Base spider
â”‚  â”‚   â”œâ”€â”€ spider_factory.py        # Creates domain-specific spiders
â”‚  â”‚   â”œâ”€â”€ nykaa_spider.py          
â”‚  â”‚   â”œâ”€â”€ tatacliq_spider.py       
â”‚  â”‚   â”œâ”€â”€ westside_spider.py       
â”‚  â”‚   â””â”€â”€ virgio_spider.py         
â”‚  â”œâ”€â”€ utils/                       # URL normalization and helpers
â”‚  â”œâ”€â”€ items.py                     # ProductUrlItem definition
â”‚  â”œâ”€â”€ pipelines.py                 # File writing/deduplication logic
â”‚  â”œâ”€â”€ settings.py                  # Scrapy settings
â”‚  â”œâ”€â”€ run.py                       # Main runner (invokes factory per domain)
â”œâ”€â”€ scrapy.cfg                      # Scrapy configuration
â”œâ”€â”€ .gitignore                      
â”œâ”€â”€ README.md                       
â””â”€â”€ requirements.txt                # Python dependencies
```

---

## ğŸ“¦ Setup

1. **Clone the repo**

```bash
git clone https://github.com/shivammotani/scrapy-crawler.git
cd scrapy-crawler
```

2. **Set up virtual environment (optional but recommended)**

```bash
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

---

## ğŸ§ª Running the Crawler

Use the custom runner script to loop through domains:

```bash
cd product-crawler
python run.py
```

This will:
- Load domains from settings
- Use the spider factory to pick the appropriate handler
- Start crawling and extracting product URLs

---

## ğŸ“ Output

Extracted product URLs are saved per domain via the pipeline, e.g.:

```
output/nykaa_product_urls.json
output/tatacliq_product_urls.json
output/westside_product_urls.json
output/virgio_product_urls.json
```

You can modify the storage format inside `pipelines.py`.

---

## ğŸ› ï¸ Customize

- Add domain-specific logic inside `spiders/{domain}_spider.py`
- Define matching patterns in `settings.py`:
  ```python
  PRODUCT_URL_PATTERNS = [r"/p/\d+", r"/products/"]
  COLLECTION_URL_PATTERNS = [r"/c-\w+"]
  ```

---

## ğŸ“š Requirements

- Python 3.8+
- Scrapy
- scrapy-playwright (for JS-rendered sites like TataCliq)

Install via:

```bash
pip install scrapy scrapy-playwright
playwright install
```
